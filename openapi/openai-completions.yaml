openapi: 3.0.0
info:
  title: OpenAI API
  version: 1.2.0
servers:
  - url: https://api.openai.com/v1
paths:
  /completions:
    post:
      operationId: createCompletion
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateCompletionRequest"
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateCompletionResponse"
          description: OK
      summary: Creates a completion for the provided prompt and parameters
      tags:
        - OpenAI
components:
  schemas:
    CreateCompletionRequest:
      properties:
        best_of:
          default: 1
          description: >
            Generates `best_of` completions server-side and returns the "best"
            (the one with the highest log probability per token). Results cannot
            be streamed.

            When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return â€“ `best_of` must be greater than `n`.

            **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
          maximum: 20
          minimum: 0
          nullable: true
          type: integer
        echo:
          default: false
          description: |
            Echo back the prompt in addition to the completion
          nullable: true
          type: boolean
        frequency_penalty:
          default: 0
          description: >
            Number between -2.0 and 2.0. Positive values penalize new tokens
            based on their existing frequency in the text so far, decreasing the
            model's likelihood to repeat the same line verbatim.

            [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
          maximum: 2
          minimum: -2
          nullable: true
          type: number
        logit_bias:
          default: null
          description: >
            Modify the likelihood of specified tokens appearing in the
            completion.

            Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

            As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being generated.
          nullable: true
          type: object
        logprobs:
          default: null
          description: >
            Include the log probabilities on the `logprobs` most likely tokens,
            as well the chosen tokens. For example, if `logprobs` is 5, the API
            will return a list of the 5 most likely tokens. The API will always
            return the `logprob` of the sampled token, so there may be up to
            `logprobs+1` elements in the response.

            The maximum value for `logprobs` is 5. If you need more than this, please contact us through our [Help center](https://help.openai.com) and describe your use case.
          maximum: 5
          minimum: 0
          nullable: true
          type: integer
        max_tokens:
          default: 16
          description: >
            The maximum number of [tokens](/tokenizer) to generate in the
            completion.

            The token count of your prompt plus `max_tokens` cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).
          minimum: 0
          nullable: true
          type: integer
        model:
          description: ID of the model to use. You can use the [List
            models](/docs/api-reference/models/list) API to see all of your
            available models, or see our [Model overview](/docs/models/overview)
            for descriptions of them.
          type: string
        n:
          default: 1
          description: >
            How many completions to generate for each prompt.

            **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
          maximum: 128
          minimum: 1
          nullable: true
          type: integer
        presence_penalty:
          default: 0
          description: >
            Number between -2.0 and 2.0. Positive values penalize new tokens
            based on whether they appear in the text so far, increasing the
            model's likelihood to talk about new topics.

            [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
          maximum: 2
          minimum: -2
          nullable: true
          type: number
        prompt:
          default: <|endoftext|>
          description: >
            The prompt(s) to generate completions for, encoded as a string,
            array of strings, array of tokens, or array of token arrays.

            Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.
          nullable: true
          oneOf:
            - default: ""
              type: string
            - items:
                default: ""
                type: string
              type: array
              items:
                type: integer
              minItems: 1
              type: array
              items:
                items:
                  type: integer
                minItems: 1
                type: array
              minItems: 1
              type: array
        stop:
          default: null
          description: >
            Up to 4 sequences where the API will stop generating further
            tokens. The returned text will not contain the stop sequence.
          nullable: true
          oneOf:
            - default: <|endoftext|>
              nullable: true
              type: string
            - items:
                type: string
              maxItems: 4
              minItems: 1
              type: array
        stream:
          default: false
          description: >
            Whether to stream back partial progress. If set, tokens will be
            sent as data-only [server-sent
            events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data:
            [DONE]` message.
          nullable: true
          type: boolean
        suffix:
          default: null
          description: The suffix that comes after a completion of inserted text.
          nullable: true
          type: string
        temperature:
          default: 1
          description: >
            What sampling temperature to use, between 0 and 2. Higher values
            like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.

            We generally recommend altering this or `top_p` but not both.
          maximum: 2
          minimum: 0
          nullable: true
          type: number
        top_p:
          default: 1
          description: >
            An alternative to sampling with temperature, called nucleus
            sampling, where the model considers the results of the tokens with
            top_p probability mass. So 0.1 means only the tokens comprising the
            top 10% probability mass are considered.

            We generally recommend altering this or `temperature` but not both.
          maximum: 1
          minimum: 0
          nullable: true
          type: number
        user:
          description: >
            A unique identifier representing your end-user, which can help
            OpenAI to monitor and detect abuse. [Learn
            more](/docs/guides/safety-best-practices/end-user-ids).
          type: string
      required:
        - model
      type: object
    CreateCompletionResponse:
      properties:
        choices:
          items:
            properties:
              finish_reason:
                type: string
              index:
                type: integer
              logprobs:
                nullable: true
                properties:
                  text_offset:
                    items:
                      type: integer
                    type: array
                  token_logprobs:
                    items:
                      type: number
                    type: array
                  tokens:
                    items:
                      type: string
                    type: array
                  top_logprobs:
                    items:
                      type: object
                    type: array
                type: object
              text:
                type: string
            type: object
          type: array
        created:
          type: integer
        id:
          type: string
        model:
          type: string
        object:
          type: string
        usage:
          properties:
            completion_tokens:
              type: integer
            prompt_tokens:
              type: integer
            total_tokens:
              type: integer
          required:
            - prompt_tokens
            - completion_tokens
            - total_tokens
          type: object
      required:
        - id
        - object
        - created
        - model
        - choices
      type: object
